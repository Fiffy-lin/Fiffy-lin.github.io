---
layout: post
title: "回头看 Word2Vec"
subtitle: ""
date: 2022-01-12
author: "Fiffy"
header-img: "img/post-bg-2015.jpg"
tags: ["Paper"]
---

# 前言

最近在从头开始看图的一写论文，第一篇就是 DeepWalk. 思想和 word2vec 非常像，用了 Skipgram、层级 Softmax等等东西。回头一想，这些出自 word2vec 的很多理论都还没搞明白，需要再看看。

# 两种训练方式

Word2Vec 使用了 CBOW 和 Skip-Gram 两种方式。

记词表维度为 N，隐层维度为 H，窗口大小为 C：

CBOW 使用上下文来预测中心词，输入的是上下文的多个 one-hot 向量，长度为 [ C-1, N ]，经过 [ N, H ] 的输入层后，转换为 [ C-1, H ] 维后再求平均，得到 [ 1, H ]. 这一隐层向量经过输出层 [ H, N ] 转换得到 [ 1, N ] 形状的向量，经过一个 Softmax 层获得最大概率的中心词。

Skip-Gram 使用中心词来预测上下文，预先生成 (中心词，上下文词) 的词对。输入的是中心词，目标是上下文中的某一个词。输入层形状为 [ 1, N ]，除了没有求平均操作之外，其余和 CBOW 几乎一样。

二者在训练速度上有区别。如果序列长度为 L，CBOW 会进行 L 次前向传播和长度更新，而 Skip-Gram 会对窗口内的每一个上下文都生成一个词对，训练次数为 L * (C - 1)。

Skip-Gram 对于低频词的表现据说会好一点，虽然我也没试过。由于 CBOW 是预测中心词，因此低频词会和高频词做竞争，预测出低频词的概率本来就很小。但是 Skip-Gram 预测的是低频词的上下文，就不存在这个问题。并且因为 Skip-Gram 训练次数多，也会增加效果。

# 降采样

目的是减轻词频带来的影响，少采一部分高频词。

![](http://mccormickml.com/assets/word2vec/subsample_func_plot.png)

Word2Vec 采用的降采样方法是计算某个词的丢弃概率，它是 词频/总词数 的函数。

# 层级 Softmax

Word2Vec 的输出层有 N 个节点，所以权重矩阵是 [ H, N ]，对每个词来说，都要更新 H * N 数量的参数，在 N 特别大的时候不仅是没有必要，甚至是算不了的（2014年）。

所以他们想了个办法，Softmax 不是为了计算归一化概率吗，他们把概率计算变成了个树，这个树有 N 个叶子节点，计算从根节点出发走到这个词对应叶子节点的概率。对于二叉树来说，层高就是 logN ，复杂度明显降低。但是实际上就和 Softmax 没啥关系了。

这个树每个非叶子节点都是有权重的，遇到岔路的时候，计算的是二分类。详见这个大佬的[理论推导](https://zhuanlan.zhihu.com/p/56139075)。

此外，还有一个这个树怎么建的问题。根据词频建霍夫曼树可以有效减少计算次数。

# 负采样

更简单粗暴了。最后一层更新权重的时候，Groundtruth 是必须更新的，我们再挑几个负样本一起更新，剩下的直接就不更新了算了。负样本的选取概率也是根据词频来的。
