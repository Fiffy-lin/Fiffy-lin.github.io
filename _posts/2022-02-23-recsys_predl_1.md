---
layout: post
title: "深度学习推荐系统-前深度学习时代-1"
subtitle: ""
date: 2022-02-23
author: "Fiffy"
header-img: "img/post-bg-2015.jpg"
tags: ["Book","推荐"]
---

# 协同过滤

## UserCF

实现方式：

1. 构建 user 和 item 交互的共现矩阵
2. 计算 user 和 user 的相似度矩阵
3. 计算 user 对某 item 的分数
   1. 对给定的 user，召回前 topk 相似的 user
   2. 用 topk 相似 user 对 item 的分数与 user 相似度加权得到该 item 分数
4. 根据分数决定要不要推荐

好：

- 社交性强

坏：

- user 数量较大时难以承受
- user 行为稀疏时不好用，召回出来的不一定对这个 item 有交互（酒店预订、大件商品购买）

## ItemCF

实现方式：

1. 构建 user 和 item 交互的共现矩阵
2. 计算 item 和 item 的相似度矩阵
3. 计算 user 的推荐列表
   1. 对给定的 user，取得其交互过的数个 item
   2. 对每个 item 召回 topk 个相似物品
   3. 按照相似度排序

好：

- 兴趣稳定的领域很适合

## CF 方式容易受头部 item 影响

主要因为 multi-hot 方式表示的用户行为在 item 方面太过稀疏，导致很多情况下点积相似度为 0，但实际上并不会完全不相似。

对于头部 item 或头部 user，有着大量的交互行为，则其对几乎所有 item 或 user 都有很高的相似度。

这也就意味着 CF-based 方法对稀疏向量的处理能力较差，泛化能力不行。

# 矩阵分解

方法的提出是为了解决 CF-based 方法对稀疏向量表现不好的问题。

通过把这个稀疏的 mxn 矩阵，拆成稠密的 mxk 和 kxn 矩阵就能解决问题。

实现方式：

1. 构建 user item 共现矩阵
2. 拆出 user 和 item 矩阵
   - 方法有特征值分解、奇异值分解、梯度下降
     - 特征值分解只能用于方阵，pass
     - 奇异值分解要求输入阵稠密，需要人工填充。并且计算复杂度达到了 `O(mn^2)` ，pass
     - 梯度下降优化的目标是原始评分与 user/item 向量内积相差足够小，可以用 MSE Loss 优化，相对来说比较好用
   - 训练的时候为了避免过拟合一般要加入正则项
   - 需要考虑消除主观性的评分因素。一般引入全局偏差、item 偏差、user 偏差三个修正
3. 直接用 user 和 item 的特征向量计算相似度进行推荐

好：

- 泛化能力强，不至于很多 pair 之间的相似度是 0 
- 空间复杂度降低明显，每个 user/item 存一条向量，复杂度 `m^2` `n^2` => `k(m+n)`
- 向量实际上就是 embedding，方便组合拼接和下游使用，灵活度极高

坏：

- 只考虑了用户行为和打分，完全没有引入 user 和 item 本身的特征

# 逻辑回归

有时候我们能获取到物品的特征，那么不用这些特征就会很浪费。

逻辑回归把推荐问题转化成了 CTR 预估的问题，这样通过用户行为就能得到天然的训练数据。根据 item 的 CTR 进行排序即可得到推荐序列。

实现方式：

1. 构建 user/item 对，输入的 X 为 user/item 的特征，Y 为标签类型（正或负）
2. 在数据集上进行训练
   - 常用梯度下降法进行训练
   - f(x) = sigmoid(sum(WiXi))
3. 对 user/item 对进行预测，输出即为 CTR

好：

- 用到了特征
- 可解释性非常好，特征间的加权组合
- sigmoid 激活函数决定输出值就是概率
- 计算效率快，好部署

坏：

- 只用了一阶特征，特征间彼此是独立的。没有用到特征间的组合
- 上面那条导致了对特征设计的要求比较高
