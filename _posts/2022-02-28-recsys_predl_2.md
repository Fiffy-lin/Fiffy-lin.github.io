---
layout: post
title: "深度学习推荐系统-前深度学习时代-2"
subtitle: ""
date: 2022-02-28
author: "Fiffy"
header-img: "img/post-bg-2015.jpg"
tags: ["Book","推荐"]
---

# 特征交叉方法

前面提到的模型都只能建模一阶特征，通过他们的线性组合来得到结果。无法建模二阶特征的影响。

搬运一个书上的例子：

| 男性用户     |      |      |        |
| ------------ | ---- | ---- | ------ |
| 视频         | 点击 | 曝光 | 点击率 |
| A            | 8    | 530  | 1.51%  |
| B            | 51   | 1520 | 3.36%  |
| **女性用户** |      |      |        |
| 视频         | 点击 | 曝光 | 点击率 |
| A            | 201  | 2510 | 8.01%  |
| B            | 92   | 1010 | 9.11%  |

可以看出这两个视频都是女性向视频，并且视频 B 的点击率高于视频 A。

但是如果我们忽略性别维度：

| 视频 | 点击 | 曝光 | 点击率 |
| ---- | ---- | ---- | ------ |
| A    | 209  | 3040 | 6.88%  |
| B    | 143  | 2530 | 5.65%  |

点击率直接反转了。这是因为视频 B 在男性用户那边大量曝光，且男性用户的点击率远低于女性用户，导致了合并后视频 B 的点击率被拉低。

通过这个例子可以看出来特征间的非线性组合是有必要的。

## POLY2

非常暴力的二阶线性组合方法。

`f(x) = Sum(Sum(Wi,j * Xi * Xj))`

对每一对组合特征设置一个权重，然后全都加起来得到最终分数。

好：

- 考虑了特征组合

坏：

- 参数量爆炸，n^2
- 特征稀疏的时候做了很多无用功，特别是 one-hot 的时候
- 权重只能被样本对精确更新，对于没见过的组合一点办法没有

## Factorization Machine

FM 为每个参数维护一个 k 维矩阵，把 POLY2 的二阶参数矩阵变成了参数隐向量相乘。

`f(x) = Sum(Sum(Wi * Wj * Xi * Xj))`

好：

- 参数量大大下降，n*2 => nk
- 训练速度大大提高，梯度下降时间复杂度也降到 nk 级
- 泛化能力大大提高，因为采用特征向量，对于模型没见过的 (A, B) 组合，A 的向量可以由见过的 (A, X) 更新到，B 的向量也可以由 (B, Y) 更新到。不至于对 (A, B) 完全无能为力。

坏：

- 相比 POLY2，泛化能力提高的同时，失去了一部分精确刻画的能力。

## Field-aware Factorization Machine

比 FM 多了一个特征域的概念。

假设特征是 2 维的 Gender, 3 维的 Brand, 4 维的 City，则特征域共有三个。Gender 中的 Male 特征会维护三份隐向量，分别在和 Gender/Brand/City 的另一个特征交叉时使用。

`f(x) = Sum(Sum(Wi,j * Wj,i * Xi * Xj))`

好：

- 表达能力进一步提升

坏：

- 较高的训练复杂度，kn^2
- 较高的参数量，knf

# 特征工程模型化

## GBDT-LR

把原始信息直接塞进 GBDT，然后把 GBDT 所有子树的输出 concat 起来，作为 LR 的特征输入。GBDT 和 LR 两部分分开训练。

摆脱了之前手动选择特征的不便之处，由模型自动完成特征工程。是一个 end2end 的解决方案。

好：

- 开创了特征工程模型化的时代，工程师们不用手动去做特征清理和组合，也不用费尽心思从模型结构上做特征的融合。
- 预测一步解决

坏：

- GBDT 容易过拟合
- GBDT 作为树模型容易丢失数值信息

# 近似深度学习方法

## LS-PLM/MLR

阿里提出的一种分块的逻辑回归方法，思路来源于在预测某些种类商品对某一群用户的 CTR 时，不希望其被其他商品影响。

结构上先用 softmax 对样本分成 m 类，用分类分数乘上本类的逻辑回归结果，加起来得到总结果。可以近似看作是一个三层神经网络：输入层、维度为 m 的隐层、输出层。

`f(x) = Sum(Softmax(Xi)*Sigmoid(Xi))`

好：

- end2end
- 类 attention 机制，softmax 的输出就是类别打分，再用打分加权融合 LR 结果
- 稀疏性强

# 总结

| 模型    | 原理                                                         | 特点                                       | 局限                                         |
| ------- | ------------------------------------------------------------ | ------------------------------------------ | -------------------------------------------- |
| CF      | 基于用户行为生成共现矩阵，基于相似性进行推荐                 | 简单直接                                   | 泛化能力差，处理稀疏矩阵能力差，头部效应明显 |
| MF      | 共现矩阵分解为用户矩阵和物品矩阵，点积相似度推荐             | 泛化能力和稀疏矩阵处理能力增强             | 无法融合用户和物品特征                       |
| LR      | 输入用户和物品特征，把推荐转化为 CTR 预估问题                | 能融合多类特征                             | 不具备特征交叉组合能力                       |
| FM      | 加入二阶交叉，为特征引入隐向量                               | 能做特征交叉，表达能力增强                 | 组合爆炸限制了无法做三维特征交叉方法         |
| FFM     | 增加特征域概念，特征和不同特征域的另一特征交叉时使用不同隐向量 | 进一步增强模型表达能力                     | 训练开销过大                                 |
| GBDT-LR | 使用 GBDT 自动完成特征工程，GBDT 的输出作为 LR 的输入        | 特征工程模型化，具备更强的高阶特征组合能力 | GBDT 无法并行训练，更新所需时间较长          |
| LS-PLM  | 为训练样本分片，每个片单独计算 LR 结果，再加权组合           | 结构类似三层神经网络                       | 结构简单，还有优化空间                       |

